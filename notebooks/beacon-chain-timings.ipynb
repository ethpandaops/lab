{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 16:31:53,437 - beacon-chain-timings - INFO - Good to go!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lib import Lab\n",
    "\n",
    "# Initialize lab\n",
    "lab = Lab('beacon-chain-timings', '../config.yaml')\n",
    "lab.setup()\n",
    "lab.setup_pandaops_clickhouse()\n",
    "log = lab.log\n",
    "\n",
    "# Get notebook specific config\n",
    "notebook_config = lab.get_notebook_config()\n",
    "\n",
    "writer = lab.get_data_writer()\n",
    "\n",
    "pandaops_clickhouse_client = lab.get_pandaops_clickhouse_client()\n",
    "\n",
    "## Clear the data directory\n",
    "lab.delete_directory('')\n",
    "\n",
    "log.info(\"Good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeaconChainTimings(time_windows=[TimeWindow(file='last_30_days', step='6h', label='Last 30d', range='-30d'), TimeWindow(file='last_90_days', step='1d', label='Last 90d', range='-90d')], networks=['mainnet', 'sepolia', 'holesky'], data_dir='../data/beacon-chain-timings')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beacon_chain_timings_config = lab.get_notebook_config().as_beacon_chain_timings()\n",
    "beacon_chain_timings_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 16:56:54,199 - beacon-chain-timings - INFO - Fetching data for last_30_days\n",
      "2025-01-10 16:56:55,844 - beacon-chain-timings - INFO - Found 363 entries for time window last_30_days\n",
      "2025-01-10 16:56:55,848 - beacon-chain-timings - INFO - Fetching data for last_90_days\n",
      "2025-01-10 16:56:56,731 - beacon-chain-timings - INFO - Found 273 entries for time window last_90_days\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "from datetime import datetime, timezone\n",
    "query = text(\"\"\"\n",
    "    WITH time_slots AS (\n",
    "        SELECT \n",
    "            toStartOfInterval(slot_start_date_time, INTERVAL :step_seconds second) as time_slot,\n",
    "            meta_network_name,\n",
    "            min(propagation_slot_start_diff) as min_arrival,\n",
    "            max(propagation_slot_start_diff) as max_arrival,\n",
    "            avg(propagation_slot_start_diff) as avg_arrival,\n",
    "            quantile(0.05)(propagation_slot_start_diff) as p05_arrival,\n",
    "            quantile(0.50)(propagation_slot_start_diff) as p50_arrival,\n",
    "            quantile(0.95)(propagation_slot_start_diff) as p95_arrival,\n",
    "            count(*) as total_blocks\n",
    "        FROM beacon_api_eth_v1_events_block FINAL\n",
    "        WHERE\n",
    "            slot_start_date_time BETWEEN toDateTime(:start_date) AND toDateTime(:end_date)\n",
    "            AND meta_network_name IN (:networks)\n",
    "            AND propagation_slot_start_diff < 6000\n",
    "        GROUP BY time_slot, meta_network_name\n",
    "    )\n",
    "    SELECT\n",
    "        time_slot as time,\n",
    "        meta_network_name,\n",
    "        min_arrival,\n",
    "        max_arrival,\n",
    "        avg_arrival,\n",
    "        p05_arrival,\n",
    "        p50_arrival,\n",
    "        p95_arrival,\n",
    "        total_blocks\n",
    "    FROM time_slots\n",
    "    ORDER BY time_slot ASC\n",
    "\"\"\")\n",
    "\n",
    "for window in beacon_chain_timings_config.time_windows:\n",
    "    start_date, end_date = window.get_time_range(datetime.now(timezone.utc))\n",
    "    step_seconds = window.get_step_seconds()\n",
    "    \n",
    "    start_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    log.info(f\"Fetching data for {window.file}\")\n",
    "    \n",
    "    result = pandaops_clickhouse_client.execute(\n",
    "        query,\n",
    "        {\n",
    "            \"start_date\": start_str,\n",
    "            \"end_date\": end_str,\n",
    "            \"networks\": beacon_chain_timings_config.networks,\n",
    "            \"step_seconds\": step_seconds\n",
    "        }\n",
    "    )\n",
    "    timings = result.fetchall()\n",
    "\n",
    "    if len(timings) == 0:\n",
    "        log.warning(f\"No data found for time window {window.file}\")\n",
    "        continue\n",
    "\n",
    "    log.info(f\"Found {len(timings)} entries for time window {window.file}\")\n",
    "    \n",
    "    # Process each network separately\n",
    "    for network in beacon_chain_timings_config.networks:\n",
    "        network_timings = [t for t in timings if t[1] == network]\n",
    "        if not network_timings:\n",
    "            continue\n",
    "            \n",
    "        # Structure data as arrays to save space\n",
    "        times = []\n",
    "        mins = []\n",
    "        maxs = []\n",
    "        avgs = []\n",
    "        p05s = []\n",
    "        p50s = []\n",
    "        p95s = []\n",
    "        blocks = []\n",
    "        \n",
    "        for t in network_timings:\n",
    "            times.append(int(t[0].timestamp()))\n",
    "            mins.append(round(t[2], 3))\n",
    "            maxs.append(round(t[3], 3))\n",
    "            avgs.append(round(t[4], 3))\n",
    "            p05s.append(round(t[5], 3))\n",
    "            p50s.append(round(t[6], 3))\n",
    "            p95s.append(round(t[7], 3))\n",
    "            blocks.append(t[8])\n",
    "            \n",
    "        # Write compact array format\n",
    "        formatted_data = {\n",
    "            \"timestamps\": times,\n",
    "            \"mins\": mins,\n",
    "            \"maxs\": maxs,\n",
    "            \"avgs\": avgs,\n",
    "            \"p05s\": p05s,\n",
    "            \"p50s\": p50s,\n",
    "            \"p95s\": p95s,\n",
    "            \"blocks\": blocks\n",
    "        }\n",
    "            \n",
    "        # Write to file per time window and network\n",
    "        lab.write_json(f\"block_timings/{network}/{window.file}.json\", formatted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 17:38:04,941 - beacon-chain-timings - INFO - Fetching block and blob data for last_30_days (2024-12-11 07:38:04 to 2025-01-10 07:38:04)\n",
      "2025-01-10 17:38:04,942 - beacon-chain-timings - INFO - Querying blob data...\n",
      "2025-01-10 17:38:09,475 - beacon-chain-timings - INFO - Found blob data for 503400 slots\n",
      "2025-01-10 17:38:09,476 - beacon-chain-timings - INFO - Querying MEV relay data...\n",
      "2025-01-10 17:38:11,216 - beacon-chain-timings - INFO - Found 264044 MEV relay slots\n",
      "2025-01-10 17:38:11,216 - beacon-chain-timings - INFO - Querying block arrival data...\n",
      "2025-01-10 17:38:11,217 - beacon-chain-timings - INFO - Querying block size data...\n",
      "2025-01-10 17:38:17,907 - beacon-chain-timings - INFO - Found arrival data for 621757 blocks\n",
      "2025-01-10 17:38:22,072 - beacon-chain-timings - INFO - Found size data for 619833 blocks\n",
      "2025-01-10 17:38:22,073 - beacon-chain-timings - INFO - Getting proposer entities...\n",
      "2025-01-10 17:38:31,121 - beacon-chain-timings - INFO - Merged data contains 619833 blocks\n",
      "2025-01-10 17:38:31,121 - beacon-chain-timings - INFO - Processing network mainnet...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'solo_stakers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 143\u001b[0m\n\u001b[1;32m    141\u001b[0m network_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_mev\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m network_df\u001b[38;5;241m.\u001b[39mslot\u001b[38;5;241m.\u001b[39misin(mev_slots)\n\u001b[1;32m    142\u001b[0m network_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(network_df, proposer_entities, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproposer_index\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m network_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_solo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m network_df\u001b[38;5;241m.\u001b[39mentity[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolo_stakers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Bucket sizes into 32KB chunks and get average arrival time per bucket\u001b[39;00m\n\u001b[1;32m    146\u001b[0m network_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize_bucket\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (network_df\u001b[38;5;241m.\u001b[39mtotal_size \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m))\u001b[38;5;241m.\u001b[39mround() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/envs/ldm/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/envs/ldm/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/envs/ldm/lib/python3.10/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'solo_stakers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fetch block sizes, blob data and build CDF for each time window\n",
    "for window in beacon_chain_timings_config.time_windows:\n",
    "    start_date, end_date = window.get_time_range(datetime.now(timezone.utc))\n",
    "    start_str = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_str = end_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    log.info(f\"Fetching block and blob data for {window.file} ({start_str} to {end_str})\")\n",
    "\n",
    "    # Get blob data\n",
    "    log.info(\"Querying blob data...\")\n",
    "    blob_query = text(\"\"\"\n",
    "        SELECT\n",
    "            slot,\n",
    "            COUNT(*) * 131072 as total_blob_bytes -- 128KB per blob\n",
    "        FROM canonical_beacon_blob_sidecar FINAL\n",
    "        WHERE\n",
    "            slot_start_date_time BETWEEN toDateTime(:start_date) AND toDateTime(:end_date)\n",
    "            AND meta_network_name IN (:networks)\n",
    "        GROUP BY slot\n",
    "    \"\"\")\n",
    "\n",
    "    blob_result = pandaops_clickhouse_client.execute(\n",
    "        blob_query,\n",
    "        {\n",
    "            \"start_date\": start_str,\n",
    "            \"end_date\": end_str,\n",
    "            \"networks\": beacon_chain_timings_config.networks\n",
    "        }\n",
    "    )\n",
    "    blob_data = {r[0]: r[1] for r in blob_result.fetchall()}\n",
    "    log.info(f\"Found blob data for {len(blob_data)} slots\")\n",
    "\n",
    "    # Get MEV relay data\n",
    "    log.info(\"Querying MEV relay data...\")\n",
    "    mev_query = text(\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            slot\n",
    "        FROM mev_relay_proposer_payload_delivered FINAL\n",
    "        WHERE\n",
    "            slot_start_date_time BETWEEN toDateTime(:start_date) AND toDateTime(:end_date)\n",
    "            AND meta_network_name IN (:networks)\n",
    "    \"\"\")\n",
    "    \n",
    "    mev_result = pandaops_clickhouse_client.execute(\n",
    "        mev_query,\n",
    "        {\n",
    "            \"start_date\": start_str,\n",
    "            \"end_date\": end_str,\n",
    "            \"networks\": beacon_chain_timings_config.networks\n",
    "        }\n",
    "    )\n",
    "    mev_slots = set(r[0] for r in mev_result.fetchall())\n",
    "    log.info(f\"Found {len(mev_slots)} MEV relay slots\")\n",
    "\n",
    "    # Get block arrival data\n",
    "    log.info(\"Querying block arrival data...\")\n",
    "    block_arrival_query = text(\"\"\"\n",
    "        SELECT \n",
    "            slot,\n",
    "            meta_network_name,\n",
    "            min(propagation_slot_start_diff) as arrival_time\n",
    "        FROM beacon_api_eth_v1_events_block FINAL\n",
    "        WHERE\n",
    "            slot_start_date_time BETWEEN toDateTime(:start_date) AND toDateTime(:end_date)\n",
    "            AND meta_network_name IN (:networks)\n",
    "        GROUP BY slot, meta_network_name\n",
    "    \"\"\")\n",
    "\n",
    "    # Get block size data\n",
    "    log.info(\"Querying block size data...\")\n",
    "    block_size_query = text(\"\"\"\n",
    "        SELECT \n",
    "            slot,\n",
    "            meta_network_name,\n",
    "            proposer_index,\n",
    "            block_total_bytes_compressed\n",
    "        FROM canonical_beacon_block FINAL\n",
    "        WHERE\n",
    "            slot_start_date_time BETWEEN toDateTime(:start_date) AND toDateTime(:end_date)\n",
    "            AND meta_network_name IN (:networks)\n",
    "    \"\"\")\n",
    "\n",
    "    params = {\n",
    "        \"start_date\": start_str,\n",
    "        \"end_date\": end_str,\n",
    "        \"networks\": beacon_chain_timings_config.networks\n",
    "    }\n",
    "\n",
    "    # Execute queries and convert to dataframes\n",
    "    arrival_df = pd.DataFrame(\n",
    "        pandaops_clickhouse_client.execute(block_arrival_query, params).fetchall(),\n",
    "        columns=['slot', 'meta_network_name', 'arrival_time']\n",
    "    )\n",
    "    log.info(f\"Found arrival data for {len(arrival_df)} blocks\")\n",
    "    \n",
    "    size_df = pd.DataFrame(\n",
    "        pandaops_clickhouse_client.execute(block_size_query, params).fetchall(),\n",
    "        columns=['slot', 'meta_network_name', 'proposer_index', 'block_size']\n",
    "    )\n",
    "    log.info(f\"Found size data for {len(size_df)} blocks\")\n",
    "\n",
    "    # Get proposer entities\n",
    "    log.info(\"Getting proposer entities...\")\n",
    "    proposer_query = text(\"\"\"\n",
    "        SELECT \n",
    "            `index` as proposer_index,\n",
    "            entity\n",
    "        FROM ethseer_validator_entity\n",
    "        WHERE \n",
    "            meta_network_name IN (:networks)\n",
    "    \"\"\")\n",
    "    proposer_entities = pd.DataFrame(\n",
    "        pandaops_clickhouse_client.execute(proposer_query, params).fetchall(),\n",
    "        columns=['proposer_index', 'entity']\n",
    "    )\n",
    "\n",
    "    # Merge dataframes and only keep slots that exist in size_df (canonical blocks)\n",
    "    block_data = pd.merge(\n",
    "        arrival_df, \n",
    "        size_df,\n",
    "        on=['slot', 'meta_network_name'],\n",
    "        how='right'\n",
    "    ).dropna()\n",
    "    log.info(f\"Merged data contains {len(block_data)} blocks\")\n",
    "\n",
    "    # Process each network\n",
    "    for network in beacon_chain_timings_config.networks:\n",
    "        log.info(f\"Processing network {network}...\")\n",
    "        network_df = block_data[block_data.meta_network_name == network].copy()\n",
    "        if network_df.empty:\n",
    "            log.warning(f\"No data found for network {network}\")\n",
    "            continue\n",
    "            \n",
    "        # Add blob sizes, MEV flag and entity info\n",
    "        network_df['total_size'] = network_df.apply(\n",
    "            lambda row: max(row.block_size + blob_data.get(row.slot, 0), 1),  # Ensure minimum size of 1 byte\n",
    "            axis=1\n",
    "        )\n",
    "        network_df['is_mev'] = network_df.slot.isin(mev_slots)\n",
    "        network_df = pd.merge(network_df, proposer_entities, on='proposer_index', how='left')\n",
    "        network_df['is_solo'] = network_df.entity == 'solo_stakers'\n",
    "\n",
    "        # Bucket sizes into 32KB chunks and get average arrival time per bucket\n",
    "        network_df['size_bucket'] = (network_df.total_size / (32 * 1024)).round() * 32\n",
    "        network_df['size_bucket'] = network_df['size_bucket'].apply(lambda x: max(x, 32))  # Minimum bucket of 32KB\n",
    "        \n",
    "        # Calculate averages for all blocks, MEV blocks, non-MEV blocks, and solo staker blocks\n",
    "        avg_all = network_df.groupby('size_bucket')['arrival_time'].mean().round().reset_index()\n",
    "        avg_mev = network_df[network_df.is_mev].groupby('size_bucket')['arrival_time'].mean().round().reset_index()\n",
    "        avg_non_mev = network_df[~network_df.is_mev].groupby('size_bucket')['arrival_time'].mean().round().reset_index()\n",
    "        avg_solo_mev = network_df[network_df.is_solo & network_df.is_mev].groupby('size_bucket')['arrival_time'].mean().round().reset_index()\n",
    "        avg_solo_non_mev = network_df[network_df.is_solo & ~network_df.is_mev].groupby('size_bucket')['arrival_time'].mean().round().reset_index()\n",
    "\n",
    "        # Write data\n",
    "        formatted_data = {\n",
    "            \"sizes_kb\": avg_all.size_bucket.tolist(),\n",
    "            \"arrival_times_ms\": {\n",
    "                \"all\": avg_all.arrival_time.tolist(),\n",
    "                \"mev\": avg_mev.arrival_time.tolist() if not avg_mev.empty else [],\n",
    "                \"non_mev\": avg_non_mev.arrival_time.tolist() if not avg_non_mev.empty else [],\n",
    "                \"solo_mev\": avg_solo_mev.arrival_time.tolist() if not avg_solo_mev.empty else [],\n",
    "                \"solo_non_mev\": avg_solo_non_mev.arrival_time.tolist() if not avg_solo_non_mev.empty else []\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output_path = f\"size_cdf/{network}/{window.file}.json\"\n",
    "        log.info(f\"Writing data to {output_path}\")\n",
    "        lab.write_json(output_path, formatted_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
